# IG Follower Analyzer - Cloud-Native Architecture ğŸš€

Complete cloud-based system for managing Instagram client following lists, with a web dashboard for VAs.

## What's New?

### Old System (File-Based)
- âŒ Manual CLI on local machine
- âŒ JSON file storage (132MB, unscalable)
- âŒ Manual Railway syncs
- âŒ Single-user, local-only
- âŒ No real-time updates

### New System (Cloud-Native)
- âœ… Modern web dashboard (accessible anywhere)
- âœ… Managed database (Supabase PostgreSQL)
- âœ… Background workers (automatic scraping)
- âœ… REST API (programmatic access)
- âœ… Multi-user ready
- âœ… Real-time job monitoring
- âœ… Auto-deploy via GitHub
- âœ… Scalable to 1000s of clients

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FRONTEND                             â”‚
â”‚  Next.js Web UI (Vercel) - VA Dashboard                  â”‚
â”‚  â€¢ Add clients                                           â”‚
â”‚  â€¢ Trigger scrapes                                       â”‚
â”‚  â€¢ Browse pages                                          â”‚
â”‚  â€¢ Monitor jobs                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTPS/REST API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      BACKEND API                          â”‚
â”‚  FastAPI (Render) - Business Logic                       â”‚
â”‚  â€¢ /api/clients - CRUD operations                        â”‚
â”‚  â€¢ /api/pages - Browse & filter                          â”‚
â”‚  â€¢ /api/scrapes - Queue & monitor jobs                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ Queue jobs
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BACKGROUND WORKERS                     â”‚
â”‚  Python Workers (Render)                                 â”‚
â”‚  â€¢ Client Following Worker - Scrapes following lists     â”‚
â”‚  â€¢ Profile Scraper Worker - Scrapes page details         â”‚
â”‚  Poll database for jobs â†’ Execute â†’ Update results       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ Read/Write
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DATABASE                             â”‚
â”‚  Supabase PostgreSQL                                     â”‚
â”‚  â€¢ clients - Client records                              â”‚
â”‚  â€¢ pages - Instagram pages                               â”‚
â”‚  â€¢ client_following - Relationships                      â”‚
â”‚  â€¢ scrape_runs - Job queue & history                     â”‚
â”‚  â€¢ page_profiles - Detailed page data                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–²
                     â”‚ API calls
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EXTERNAL SERVICES                      â”‚
â”‚  â€¢ Apify - Instagram scraping                            â”‚
â”‚  â€¢ GitHub - Code hosting & CI/CD                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure

```
ig-follower-analyzer/
â”œâ”€â”€ api/                      # FastAPI Backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py          # API entry point
â”‚   â”‚   â”œâ”€â”€ config.py        # Configuration
â”‚   â”‚   â”œâ”€â”€ db.py            # Supabase connection
â”‚   â”‚   â”œâ”€â”€ routes/          # API endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ clients.py   # Client CRUD
â”‚   â”‚   â”‚   â”œâ”€â”€ pages.py     # Page browsing
â”‚   â”‚   â”‚   â””â”€â”€ scrapes.py   # Scrape job management
â”‚   â”‚   â”œâ”€â”€ schemas/         # Pydantic models
â”‚   â”‚   â””â”€â”€ services/        # Business logic
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ workers/                  # Background Workers
â”‚   â”œâ”€â”€ client_following_worker.py  # Scrapes following lists
â”‚   â”œâ”€â”€ profile_scrape_worker.py    # Scrapes profile details
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ web/                      # Next.js Web UI
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ page.tsx         # Main dashboard
â”‚   â”‚   â”œâ”€â”€ layout.tsx       # Root layout
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ClientsTab.tsx    # Client management
â”‚   â”‚   â”‚   â”œâ”€â”€ PagesTab.tsx      # Page browsing
â”‚   â”‚   â”‚   â””â”€â”€ ScrapesTab.tsx    # Job monitoring
â”‚   â”‚   â””â”€â”€ lib/
â”‚   â”‚       â””â”€â”€ api.ts       # API client
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ next.config.js
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ scripts/                  # Migration & Utilities
â”‚   â”œâ”€â”€ migrate_clients_json.py   # JSON â†’ Supabase migration
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ docs/                     # Documentation
â”‚   â”œâ”€â”€ CURRENT_ARCHITECTURE.md   # Old system overview
â”‚   â”œâ”€â”€ DATA_MODEL_SUPABASE.md    # Database schema
â”‚   â”œâ”€â”€ DEPLOYMENT.md             # Deploy guide
â”‚   â”œâ”€â”€ MONITORING.md             # Monitoring setup
â”‚   â””â”€â”€ CLOUD_ARCHITECTURE_README.md  # This file
â”‚
â”œâ”€â”€ .github/workflows/        # CI/CD
â”‚   â””â”€â”€ ci.yml               # GitHub Actions pipeline
â”‚
â”œâ”€â”€ docker-compose.yml        # Local development setup
â”œâ”€â”€ env.template             # Environment variable template
â””â”€â”€ README.md                # Project overview

# Legacy files (still present, but replaced by new system)
â”œâ”€â”€ main.py                  # Old CLI (deprecated)
â”œâ”€â”€ categorize_app.py        # Old Streamlit UI (deprecated)
â”œâ”€â”€ scrape_profiles.py       # Old scraper (replaced by workers)
â””â”€â”€ clients_data.json        # Old storage (migrated to DB)
```

## Tech Stack

### Frontend
- **Next.js 14** - React framework with App Router
- **TypeScript** - Type safety
- **Tailwind CSS** - Styling
- **React Query** - Data fetching & caching
- **Vercel** - Hosting (auto-deploy from GitHub)

### Backend
- **FastAPI** - Modern Python web framework
- **Supabase** - PostgreSQL database + REST API
- **Pydantic** - Data validation
- **Render** - Hosting (API + workers)

### Workers
- **Python 3.11** - Worker scripts
- **Apify Client** - Instagram scraping
- **Docker** - Containerization
- **Render** - Background worker hosting

### Infrastructure
- **GitHub Actions** - CI/CD pipeline
- **Docker Compose** - Local development
- **Environment Variables** - Secret management

## Getting Started

### Prerequisites

1. **Supabase Account** (free)
   - Create project: https://supabase.com
   - Note Project URL and service_role key

2. **Apify Account**
   - Sign up: https://apify.com
   - Add $5-10 credits
   - Get API token

3. **GitHub Account**
   - For code hosting and CI/CD

4. **Hosting Accounts** (all free tiers available)
   - Vercel (web UI)
   - Render (API + workers)

### Local Development

1. **Clone Repository**
   ```bash
   git clone https://github.com/your-username/ig-follower-analyzer.git
   cd ig-follower-analyzer
   ```

2. **Set Environment Variables**
   ```bash
   cp env.template .env
   # Edit .env with your credentials
   ```

3. **Run with Docker Compose**
   ```bash
   docker-compose up
   ```

   This starts:
   - API on http://localhost:8000
   - Web UI on http://localhost:3000
   - 2 background workers

4. **Or Run Services Individually**
   
   **API:**
   ```bash
   cd api
   pip install -r requirements.txt
   uvicorn app.main:app --reload
   ```

   **Workers:**
   ```bash
   cd workers
   pip install -r requirements.txt
   python client_following_worker.py
   # In another terminal:
   python profile_scrape_worker.py
   ```

   **Web UI:**
   ```bash
   cd web
   npm install
   npm run dev
   ```

### Cloud Deployment

Follow the detailed guide: [`docs/DEPLOYMENT.md`](./DEPLOYMENT.md)

**Quick Summary:**
1. Set up Supabase database (run SQL schema)
2. Deploy API to Render
3. Deploy workers to Render (2 services)
4. Deploy web UI to Vercel
5. Migrate existing data (if you have `clients_data.json`)

### Migration from Old System

If you have existing data in `clients_data.json`:

```bash
export SUPABASE_URL="https://xxx.supabase.co"
export SUPABASE_SERVICE_KEY="your-key"
python scripts/migrate_clients_json.py
```

This will transfer all clients, pages, and relationships to Supabase.

## Key Features

### 1. Web Dashboard

Access from anywhere at your Vercel URL:

**Clients Tab:**
- Add new clients (name + Instagram username)
- View all clients with following counts
- Select multiple clients for batch scraping
- Delete clients
- See last scraped timestamp

**Pages Tab:**
- Browse pages followed by multiple clients
- Filter by minimum client count (e.g., show pages followed by 3+ clients)
- Click page to view detailed profile:
  - Profile picture
  - Bio
  - Contact email
  - Promo status (warm/unknown)
  - Recent posts grid
- See follower counts and verification badges

**Scrape Jobs Tab:**
- Real-time job monitoring (auto-refresh every 5s)
- See job status: pending â†’ processing â†’ completed/failed
- View scrape results:
  - Accounts scraped
  - Coverage % (with color-coded alerts)
  - Failed usernames
- Job history with timestamps

### 2. Automated Background Processing

Workers continuously poll for jobs and execute automatically:

- **Client Following Worker**: Scrapes who clients follow
  - Gets total following count from Instagram
  - Scrapes via Apify
  - Calculates coverage % (target: â‰¥95%)
  - Stores results in database
  - Updates client record

- **Profile Scraper Worker**: Scrapes page details
  - Gets profile picture, bio, recent posts
  - Downloads and encodes images as base64
  - Detects promo openness (keywords in bio)
  - Extracts contact emails
  - Stores in `page_profiles` table

### 3. REST API

Programmatic access for integrations:

**Endpoints:**
- `GET /api/clients` - List all clients
- `POST /api/clients` - Add client
- `DELETE /api/clients/:id` - Delete client
- `GET /api/pages?min_client_count=2` - List pages
- `GET /api/pages/:id/profile` - Get profile details
- `GET /api/scrapes` - List scrape jobs
- `POST /api/scrapes/client-following` - Queue client scrape
- `POST /api/scrapes/profile-scrape` - Queue profile scrape

Full API docs: http://your-api-url.com/docs

### 4. Coverage Tracking

Every client scrape now tracks coverage:
- Gets Instagram's reported following count
- Compares to accounts retrieved
- Flags if coverage <95%
- Automatically retries if coverage <90%

Example output:
```
âœ… Excellent! Got 99.8% of accounts (â‰¥95% threshold)
```

### 5. Real-Time Monitoring

- Job status updates every 5 seconds
- See which jobs are processing
- View results immediately when complete
- Detailed error messages for failures

## Monitoring & Alerts

Set up monitoring for production:

1. **Health Checks**
   - Use UptimeRobot (free) to ping API `/health` endpoint
   - Get alerts if API goes down

2. **Coverage Alerts**
   - Workers can send Discord/Slack webhooks when coverage <95%
   - Set environment variables: `DISCORD_WEBHOOK_URL` or `SLACK_WEBHOOK_URL`

3. **Platform Monitoring**
   - Render dashboard: CPU, memory, logs
   - Vercel analytics: Page loads, errors
   - Supabase reports: Database performance

Full guide: [`docs/MONITORING.md`](./MONITORING.md)

## Cost Breakdown

### Free Tier (Development)
- **Supabase**: $0 (500MB database, enough for ~5,000 pages)
- **Render API**: $0 (spins down after 15min inactivity)
- **Render Workers**: $0 Ã— 2
- **Vercel**: $0 (unlimited bandwidth)
- **Apify**: ~$0.10-0.50 per scrape (pay-per-use)

**Total**: $0/month + Apify usage

### Production (Recommended)
- **Supabase Pro**: $25/month (8GB, better performance, backups)
- **Render Starter**: $7/month Ã— 3 services = $21/month (always-on, no spin-down)
- **Vercel Pro**: $20/month (optional - custom domain, analytics)
- **Apify**: $20-50/month depending on volume

**Total**: ~$66-96/month + Apify usage

## Advantages Over Old System

### Scalability
- **Old**: Single JSON file (132MB), would break at ~10K pages
- **New**: PostgreSQL database, scales to millions of records

### Accessibility
- **Old**: CLI on local machine, VA needs remote access to your computer
- **New**: Web dashboard accessible from anywhere, just share URL

### Automation
- **Old**: Manual scrapes via terminal, manual Railway syncs
- **New**: Click "Scrape" in UI, workers handle everything automatically

### Reliability
- **Old**: If script crashes mid-scrape, data may be lost
- **New**: Jobs are queued, workers retry failures, all tracked in database

### Collaboration
- **Old**: Single-user, file conflicts if multiple people
- **New**: Multi-user ready, database handles concurrency

### Monitoring
- **Old**: Check terminal output, no history
- **New**: Real-time job monitoring, full history, coverage tracking

### Deployment
- **Old**: Manual setup, Railway volume uploads
- **New**: Push to GitHub â†’ Auto-deploys everything

## Troubleshooting

### API Not Responding
- Check Render logs: https://dashboard.render.com
- Verify environment variables are set
- Free tier may spin down (first request takes 30s to wake up)

### Workers Not Processing Jobs
- Check worker logs on Render
- Verify Supabase connection (check service key)
- Check Apify credits

### Scrapes Failing
- Verify `APIFY_TOKEN` is valid
- Check Apify credits balance
- Some Instagram accounts may be private/deleted

### Web UI Can't Connect to API
- Check `NEXT_PUBLIC_API_URL` in Vercel settings
- Make sure it points to your Render API URL
- Check CORS settings in API

## Next Steps

After deployment:

1. **Migrate Data**: Run migration script to transfer old JSON data
2. **Test Workflow**: Add a client, trigger scrape, verify results
3. **Set Up Monitoring**: Configure alerts for coverage/failures
4. **Share with VA**: Send Vercel URL, they can start using it
5. **Archive Old System**: Backup `clients_data.json`, remove old CLI scripts

## Support & Documentation

- **Deployment Guide**: [`docs/DEPLOYMENT.md`](./DEPLOYMENT.md)
- **Database Schema**: [`docs/DATA_MODEL_SUPABASE.md`](./DATA_MODEL_SUPABASE.md)
- **Monitoring Setup**: [`docs/MONITORING.md`](./MONITORING.md)
- **API Docs**: Visit `/docs` on your API URL
- **Migration Guide**: [`scripts/README.md`](../scripts/README.md)

## Contributing

To add features:
1. Create feature branch
2. Make changes
3. Push to GitHub
4. CI runs tests automatically
5. Merge to `main` â†’ Auto-deploys

## License

Free to use for your business!

---

**Built with â¤ï¸ for a scalable, cloud-native VA workflow**

