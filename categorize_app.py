"""
Streamlit App for Human-Assisted Instagram Page Categorization
"""

import streamlit as st
import json
import os
import io
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from contextlib import redirect_stdout, redirect_stderr
from categorizer import CATEGORIES
from main import IGFollowerAnalyzer
from scrape_profiles import (
    priority_scrape,
    scrape_all,
    re_scrape_priority,
    re_scrape_all,
)

# Hotlist keywords for priority categorization
# Note: Uses partial matching - "black" matches "blacksuccess", "hustl" matches "hustlersimage"
HOTLIST_KEYWORDS = [
    "hustl",      # Matches: hustlersimage, hustlingquiet, etc.
    "afri",       # Matches: african, afro-american, etc.
    "afro",       # Matches: afro-american, etc.
    "black",      # Matches: blacksuccess, blackbillionaire, etc.
    "melanin",    # Matches: melaninmagic, melaninpoppin, etc.
    "blvck",      # Alternative spelling of black
    "culture",    # Matches: culture, cultural, etc.
    "kulture",    # Alternative spelling of culture
    "brown",      # Matches: brownskin, brownbeauty, brownsuccess, etc.
    "noir",       # French for black, used in some contexts
    "ebony"       # Another term for black/dark
]

CONTACT_METHODS = ["IG DM", "Email", "Phone", "WhatsApp", "Telegram", "Other"]

PROFILE_SCRAPER_ACTIONS = {
    "priority": {
        "label": "Priority Scrape (hotlist & 2+ clients)",
        "description": "Scrape only high-value pages so VAs can review fresh data quickly.",
        "func": priority_scrape,
    },
    "all": {
        "label": "Scrape All Pages",
        "description": "Full overnight-style scrape for every page that needs base64 images.",
        "func": scrape_all,
    },
    "re_priority": {
        "label": "Force Re-scrape (Priority)",
        "description": "Ignore recent successes/failures and re-scrape priority tiers.",
        "func": re_scrape_priority,
    },
    "re_all": {
        "label": "Force Re-scrape (All Pages)",
        "description": "Force refresh of every page, even if data already exists.",
        "func": re_scrape_all,
    },
}


def load_api_credentials() -> Tuple[Optional[str], Optional[str]]:
    """Load API credentials from environment or config file"""
    apify_token = os.environ.get("APIFY_TOKEN")
    openai_key = os.environ.get("OPENAI_API_KEY")
    
    if not apify_token or not openai_key:
        try:
            import config  # type: ignore
        except ImportError:
            config = None
        else:
            if not apify_token:
                apify_token = getattr(config, "APIFY_TOKEN", None)
            if not openai_key:
                openai_key = getattr(config, "OPENAI_API_KEY", None)
    
    return apify_token, openai_key


def get_analyzer() -> Optional[IGFollowerAnalyzer]:
    """Instantiate the IG follower analyzer with the correct data path"""
    apify_token, openai_key = load_api_credentials()
    
    if not apify_token:
        st.error("‚ùå APIFY_TOKEN is not configured. Set it on Railway or in config.py.")
        return None
    
    data_path = get_data_file_path()
    
    try:
        analyzer = IGFollowerAnalyzer(apify_token, openai_key, data_file=data_path)
        return analyzer
    except Exception as e:
        st.error(f"‚ùå Could not initialize analyzer: {str(e)}")
        return None


def format_client_option(username: str, clients: Dict[str, Dict]) -> str:
    """Format how clients appear in select boxes"""
    client = clients.get(username, {})
    display_name = client.get("name") or username
    following_count = client.get("following_count", 0)
    status = "scraped" if following_count else "needs scrape"
    return f"{display_name} (@{username}) ‚Äî {following_count:,} accounts ({status})"


def capture_logs(func, *args, **kwargs) -> str:
    """Run a function and capture stdout/stderr for display in Streamlit"""
    buffer = io.StringIO()
    try:
        with redirect_stdout(buffer), redirect_stderr(buffer):
            func(*args, **kwargs)
    except SystemExit as exit_err:
        # convert to regular exception so Streamlit can handle gracefully
        raise RuntimeError(f"Process exited: {exit_err}") from exit_err
    return buffer.getvalue()


def run_client_scrape(selected_clients: List[str], skip_validation: bool = False):
    """Run the client following scraper from the browser"""
    analyzer = get_analyzer()
    if not analyzer:
        return
    
    status = st.status("Scraping clients...", expanded=True)
    logs = io.StringIO()
    
    try:
        with redirect_stdout(logs), redirect_stderr(logs):
            for username in selected_clients:
                status.write(f"üîç Scraping @{username}...")
                # Skip validation if requested (validation can give false negatives)
                analyzer.scrape_client_following(username, validate_first=not skip_validation)
        status.update(label="‚úÖ Client scraping complete", state="complete")
    except Exception as e:
        status.update(label="‚ùå Client scraping failed", state="error")
        st.error(f"Scrape failed: {str(e)}")
    finally:
        output = logs.getvalue()
        if output:
            st.text_area("Client Scraper Logs", output, height=250)


def run_profile_scrape_action(action_key: str):
    """Trigger one of the profile scraper workflows from the browser"""
    action = PROFILE_SCRAPER_ACTIONS[action_key]
    label = action["label"]
    scrape_func = action["func"]
    data_file = get_data_file_path()
    
    status = st.status(f"{label} running...", expanded=True)
    try:
        log_output = capture_logs(scrape_func, data_file=data_file)
        status.update(label=f"{label} complete", state="complete")
    except Exception as e:
        status.update(label=f"{label} failed", state="error")
        st.error(f"{label} failed: {str(e)}")
        log_output = ""
    if log_output:
        st.text_area("Profile Scraper Logs", log_output, height=300)


def format_timestamp(ts: Optional[str]) -> str:
    """Format ISO timestamp into readable text for tables"""
    if not ts:
        return "‚Äî"
    try:
        dt = datetime.fromisoformat(ts.replace('Z', '+00:00'))
        return dt.strftime("%Y-%m-%d %H:%M")
    except Exception:
        return ts[:16] if len(ts) >= 16 else ts

# Page config
st.set_page_config(
    page_title="Instagram Page Categorizer",
    page_icon="üì±",
    layout="wide"
)

# Initialize session state
if 'current_page_idx' not in st.session_state:
    st.session_state.current_page_idx = 0
if 'selected_category' not in st.session_state:
    st.session_state.selected_category = {}
if 'data_loaded' not in st.session_state:
    st.session_state.data_loaded = False
if 'edit_page' not in st.session_state:
    st.session_state.edit_page = None
if 'edit_page_select' not in st.session_state:
    st.session_state.edit_page_select = None


def get_data_file_path() -> str:
    """Get the data file path, checking for Railway volume first"""
    # Check for Railway volume mount (persistent storage)
    volume_path = "/data/clients_data.json"
    
    # Debug: Check if we're on Railway
    if os.path.exists("/data"):
        # We're on Railway, use the volume
        # Debug: Log what we see
        if os.path.exists(volume_path):
            file_size = os.path.getsize(volume_path)
            # Only log in debug mode to avoid cluttering UI
            if os.environ.get("DEBUG_VOLUME"):
                st.caption(f"üîç DEBUG: Volume file exists, size: {file_size:,} bytes")
        return volume_path
    
    # Fall back to local file
    return "clients_data.json"


def load_data(data_file: str = None) -> Optional[Dict]:
    """Load client data from JSON file, create empty structure if it doesn't exist"""
    if data_file is None:
        data_file = get_data_file_path()
    
    if not os.path.exists(data_file):
        # File doesn't exist - DON'T create empty, return empty structure
        # This prevents overwriting data uploaded via railway CLI
        st.warning(f"‚ö†Ô∏è {data_file} not found. Returning empty structure. Upload data via 'Data Management' tab or sync from terminal.")
        return {
            "clients": {},
            "pages": {}
        }
    
    try:
        file_size = os.path.getsize(data_file)
        st.caption(f"üìä File size: {file_size:,} bytes ({file_size / 1024 / 1024:.2f} MB)")
        
        with open(data_file, 'r', encoding='utf-8') as f:
            content = f.read()
            st.caption(f"üìè Read {len(content):,} characters from file")
            
            data = json.loads(content)
            st.caption(f"‚úÖ Parsed JSON: {len(data.get('clients', {}))} clients, {len(data.get('pages', {}))} pages")
            return data
    except Exception as e:
        st.error(f"‚ùå Error loading data: {str(e)}")
        import traceback
        st.code(traceback.format_exc())
        return None


def save_data(data: Dict, data_file: str = None, sync_remote: bool = False):
    """Save client data to JSON file and optionally sync to Railway"""
    if data_file is None:
        data_file = get_data_file_path()
    data["last_updated"] = datetime.now().isoformat()
    # Ensure directory exists (for volume path)
    os.makedirs(os.path.dirname(data_file) if os.path.dirname(data_file) else ".", exist_ok=True)
    with open(data_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    # Auto-sync to Railway if requested and we're running locally (not on Railway)
    if sync_remote and not data_file.startswith('/data/'):
        try:
            # Import here to avoid issues if railway_sync isn't available
            from railway_sync import upload_via_railway_cli
            # Run sync in background thread to avoid blocking UI
            import threading
            def sync_thread():
                try:
                    # Use direct CLI upload (faster and more reliable than HTTP)
                    upload_via_railway_cli(data_file)
                except Exception as e:
                    # Silently fail - user can manually upload if needed
                    pass
            thread = threading.Thread(target=sync_thread, daemon=True)
            thread.start()
        except ImportError:
            # railway_sync not available - that's okay
            pass
        except Exception:
            # Any other error - fail silently
            pass


def matches_hotlist(page_data: Dict) -> bool:
    """Check if page matches hotlist keywords"""
    username = page_data.get("username", "").lower()
    full_name = page_data.get("full_name", "").lower()
    
    text = f"{username} {full_name}"
    return any(keyword.lower() in text for keyword in HOTLIST_KEYWORDS)


def get_pages_for_categorization(data: Dict, filter_type: str = "uncategorized", 
                                 min_clients: int = 1) -> List[Tuple[str, Dict]]:
    """Get pages sorted for categorization"""
    pages = data.get("pages", {})
    result = []
    
    for username, page_data in pages.items():
        client_count = len(page_data.get("clients_following", []))
        
        if client_count < min_clients:
            continue
        
        # Apply filter
        category = page_data.get("category")
        if filter_type == "uncategorized":
            if category and category != "UNKNOWN":
                continue
        elif filter_type == "all":
            pass  # Include all
        elif filter_type == "specific":
            # This would need a category selector - for now, show all
            pass
        
        result.append((username, page_data))
    
    # Sort: hotlist first, then by clients following, then by followers
    def sort_key(item):
        username, page_data = item
        is_hotlist = matches_hotlist(page_data)
        client_count = len(page_data.get("clients_following", []))
        follower_count = page_data.get("follower_count", 0)
        return (not is_hotlist, -client_count, -follower_count)
    
    result.sort(key=sort_key)
    return result


def get_pages_by_category(data: Dict, category: str, min_clients: int = 1) -> List[Dict]:
    """Get pages for a specific category, sorted properly"""
    pages = data.get("pages", {})
    result = []
    
    for username, page_data in pages.items():
        client_count = len(page_data.get("clients_following", []))
        
        if client_count < min_clients:
            continue
        
        page_category = page_data.get("category", "UNCATEGORIZED")
        if category == "UNCATEGORIZED":
            if page_category and page_category != "UNKNOWN":
                continue
        elif page_category != category:
            continue
        
        # Calculate metrics
        follower_count = page_data.get("follower_count", 0)
        promo_price = page_data.get("promo_price")
        
        page_info = {
            "username": username,
            "page_data": page_data,
            "client_count": client_count,
            "follower_count": follower_count,
            "promo_price": promo_price,
            "concentration": client_count / follower_count if follower_count > 0 else 0,
            "followers_per_client": follower_count / client_count if client_count > 0 else 0
        }
        
        # Calculate concentration_per_dollar if price available
        if promo_price and promo_price > 0:
            page_info["concentration_per_dollar"] = page_info["concentration"] / promo_price
        else:
            page_info["concentration_per_dollar"] = None
        
        result.append(page_info)
    
    # Sort: pages with price first (by concentration_per_dollar), then without price (by followers_per_client)
    def sort_key(page_info):
        has_price = page_info["concentration_per_dollar"] is not None
        if has_price:
            return (0, -page_info["concentration_per_dollar"])  # 0 = first group
        else:
            return (1, -page_info["followers_per_client"])  # 1 = second group
    
    result.sort(key=sort_key)
    return result


def handle_sync_api():
    """Handle /sync API endpoint via query parameters"""
    if st.query_params.get("api") == "sync" and st.query_params.get("method") == "POST":
        # This is an API call - handle it
        try:
            # Get JSON from request body (Streamlit doesn't support this directly, so we'll use a workaround)
            # For now, return instructions
            st.error("Direct API sync not supported via Streamlit. Use the Data Management tab or the terminal sync script.")
            st.stop()
        except Exception as e:
            st.error(f"Sync error: {str(e)}")
            st.stop()


def main():
    # Check for API sync request
    handle_sync_api()
    
    st.title("üì± Instagram Page Categorizer")
    st.markdown("**Human-Assisted Categorization Tool for Instagram Pages**")
    st.markdown("---")
    
    # Load data
    data_file_path = get_data_file_path()
    # Debug info
    st.caption(f"üìÅ Loading from: {data_file_path} | Exists: {os.path.exists(data_file_path)}")
    if os.path.exists(data_file_path):
        try:
            with open(data_file_path, 'r', encoding='utf-8') as f:
                preview = f.read(200)
                st.caption(f"üìÑ File preview: {preview[:100]}...")
        except Exception as e:
            st.caption(f"‚ö†Ô∏è Error reading file: {str(e)}")
    
    data = load_data()
    if data is None:
        st.stop()
    
    st.session_state.data_loaded = True
    
    # Show quick stats
    pages = data.get("pages", {})
    total_pages = len(pages)
    uncategorized = sum(1 for p in pages.values() if not p.get("category") or p.get("category") == "UNKNOWN")
    categorized = total_pages - uncategorized
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Total Pages", f"{total_pages:,}")
    with col2:
        st.metric("Categorized", f"{categorized:,}")
    with col3:
        st.metric("Uncategorized", f"{uncategorized:,}")
    
    st.markdown("---")
    
    # Main tabs - TEMPORARILY SWAPPED to test if last tab is the issue
    tab_workflow, tab1, tab2, tab3, tab4 = st.tabs([
        "üöÄ Workflow Control",
        "üîç Categorize Pages",
        "‚úèÔ∏è Edit Page Details",
        "üì§ Data Management",  # MOVED TO 4TH POSITION
        "üìÇ View by Category"   # MOVED TO LAST
    ])
    
    # TAB 0: Workflow control (add clients, run scrapers)
    with tab_workflow:
        st.header("Workflow Control Center")
        st.markdown("Add clients, run follower scrapes, and trigger profile scrapers directly from the browser.")
        
        clients = data.get("clients", {})
        
        col_wc1, col_wc2, col_wc3 = st.columns(3)
        with col_wc1:
            st.metric("Clients", len(clients))
        with col_wc2:
            st.metric("Pages", len(data.get("pages", {})))
        with col_wc3:
            apify_token, _ = load_api_credentials()
            token_status = "Configured" if apify_token else "Missing"
            st.metric("Apify Token", token_status)
        
        st.markdown("---")
        st.subheader("‚ûï Add Client")
        with st.form("add_client_form", clear_on_submit=True):
            client_name = st.text_input("Client Name", placeholder="e.g., Sarah (coaching client)")
            ig_username = st.text_input("Instagram Username", placeholder="@username")
            submitted = st.form_submit_button("Add Client")
            
            if submitted:
                normalized = ig_username.strip().lstrip('@').lower()
                if not normalized:
                    st.warning("Please enter a valid Instagram username.")
                elif normalized in clients:
                    st.warning(f"Client @{normalized} already exists.")
                else:
                    display_name = client_name.strip() if client_name else normalized
                    clients[normalized] = {
                        "name": display_name,
                        "username": normalized,
                        "added_date": datetime.now().isoformat(),
                        "following": [],
                        "following_count": 0,
                        "last_scraped": None
                    }
                    data["clients"] = clients
                    save_data(data, sync_remote=True)
                    st.success(f"‚úÖ Added client @{normalized}")
                    # Check if we're running locally (not on Railway)
                    data_file = get_data_file_path()
                    if not data_file.startswith('/data/'):
                        st.info("üîÑ Auto-syncing to Railway in the background...")
                    st.rerun()
        
        st.markdown("### üìã Client Status")
        if not clients:
            st.info("No clients added yet. Use the form above to get started.")
        else:
            client_rows = []
            for username in sorted(clients.keys()):
                client = clients[username]
                client_rows.append({
                    "Client": client.get("name") or username,
                    "Username": f"@{username}",
                    "Following Scraped": client.get("following_count", 0),
                    "Last Scraped": format_timestamp(client.get("last_scraped"))
                })
            st.dataframe(client_rows, use_container_width=True)
        
        st.markdown("---")
        st.subheader("üßπ Scrape Client Following Lists")
        if not clients:
            st.info("Add at least one client to enable scraping.")
        else:
            client_usernames = sorted(clients.keys())
            default_pending = [u for u in client_usernames if clients[u].get("following_count", 0) == 0]
            selected_clients = st.multiselect(
                "Select clients to scrape",
                client_usernames,
                default=default_pending,
                format_func=lambda u: format_client_option(u, clients),
                help="Select specific clients or use the buttons below for batch actions."
            )
            
            # Option to skip validation (validation can give false negatives)
            skip_validation = st.checkbox(
                "Skip account validation (faster, but may attempt to scrape private accounts)",
                value=False,
                help="If validation is giving false 'account does not exist' errors, enable this to skip validation and attempt scraping directly."
            )
            
            scrape_col1, scrape_col2, scrape_col3 = st.columns([1, 1, 1])
            with scrape_col1:
                if st.button("Scrape Selected Clients", type="primary"):
                    if not selected_clients:
                        st.warning("Select at least one client.")
                    else:
                        run_client_scrape(selected_clients, skip_validation=skip_validation)
            with scrape_col2:
                if st.button("Scrape All Clients"):
                    run_client_scrape(client_usernames, skip_validation=skip_validation)
            with scrape_col3:
                if st.button("Refresh Client Table"):
                    st.rerun()
        
        st.markdown("---")
        st.subheader("üì∏ Profile Scraper (Posts, Bios, Promo Signals)")
        st.caption("Runs the Apify-powered profile scraper to capture bio, highlights, posts, promo signals, and website intel.")
        
        scraper_option_labels = [
            PROFILE_SCRAPER_ACTIONS["priority"]["label"],
            PROFILE_SCRAPER_ACTIONS["all"]["label"],
            PROFILE_SCRAPER_ACTIONS["re_priority"]["label"],
            PROFILE_SCRAPER_ACTIONS["re_all"]["label"],
        ]
        option_key_map = {PROFILE_SCRAPER_ACTIONS[key]["label"]: key for key in PROFILE_SCRAPER_ACTIONS}
        
        selected_action_label = st.selectbox(
            "Choose scrape mode",
            scraper_option_labels,
            format_func=lambda label: label
        )
        
        st.caption(PROFILE_SCRAPER_ACTIONS[option_key_map[selected_action_label]]["description"])
        
        if st.button("Run Profile Scraper", key="run_profile_scraper_button"):
            action_key = option_key_map[selected_action_label]
            run_profile_scrape_action(action_key)
    
    # TAB 1: Categorize Pages
    with tab1:
        st.header("Categorize Pages")
        
        # Filter options
        col1, col2 = st.columns(2)
        with col1:
            filter_type = st.selectbox(
                "Filter Pages",
                ["uncategorized", "all", "specific"],
                format_func=lambda x: {
                    "uncategorized": "Uncategorized Only",
                    "all": "All Pages",
                    "specific": "Specific Category"
                }[x],
                index=0
            )
        
        with col2:
            min_clients = st.number_input("Minimum Clients Following", min_value=1, value=2)
        
        # Get pages
        pages_list = get_pages_for_categorization(data, filter_type, min_clients)
        
        if not pages_list:
            st.info("‚úÖ No pages found matching the filter criteria!")
            # Don't use st.stop() - it stops the entire script and prevents other tabs from rendering!
            # Just show the message and let the user continue to other tabs
            total_pages = 0
        else:
            total_pages = len(pages_list)
        
        # Only show navigation if there are pages
        if total_pages > 0:
            # Navigation
            col1, col2, col3, col4 = st.columns([1, 1, 2, 1])
            
            with col1:
                if st.button("‚óÄ Previous", disabled=st.session_state.current_page_idx == 0):
                    st.session_state.current_page_idx = max(0, st.session_state.current_page_idx - 1)
                    st.rerun()
            
            with col2:
                jump_to = st.number_input("Jump to", min_value=1, max_value=total_pages, 
                                         value=st.session_state.current_page_idx + 1, key="jump_input")
                if jump_to != st.session_state.current_page_idx + 1:
                    st.session_state.current_page_idx = jump_to - 1
                    st.rerun()
            
            with col3:
                st.markdown(f"**Page {st.session_state.current_page_idx + 1} of {total_pages}**")
            
            with col4:
                if st.button("Next ‚ñ∂", disabled=st.session_state.current_page_idx >= total_pages - 1):
                    # Save current selection before moving
                    if st.session_state.current_page_idx < len(pages_list):
                        username, _ = pages_list[st.session_state.current_page_idx]
                        selected_cat = st.session_state.selected_category.get(username)
                        if selected_cat:
                            data["pages"][username]["category"] = selected_cat
                            data["pages"][username]["last_categorized"] = datetime.now().isoformat()
                            save_data(data)
                            st.success("üíæ Saved!")
                    
                    st.session_state.current_page_idx = min(total_pages - 1, st.session_state.current_page_idx + 1)
                    st.rerun()
            
            st.markdown("---")
            
            # Display current page
            if st.session_state.current_page_idx < len(pages_list):
                username, page_data = pages_list[st.session_state.current_page_idx]
                
                # Check if hotlist
                is_hotlist = matches_hotlist(page_data)
                if is_hotlist:
                    st.markdown("### üî• **PRIORITY PAGE** (Matches hotlist keywords)")
                
                # Profile info
                col1, col2 = st.columns([1, 2])
                
                with col1:
                    # Profile picture - try base64 first, fallback to URL
                    profile_data = page_data.get("profile_data")
                    if profile_data and isinstance(profile_data, dict):
                        pic_displayed = False
                        
                        # Try base64 first (doesn't expire)
                        if profile_data.get("profile_pic_base64") and profile_data.get("profile_pic_mime_type"):
                            try:
                                data_uri = f"data:{profile_data['profile_pic_mime_type']};base64,{profile_data['profile_pic_base64']}"
                                st.image(data_uri, width=200)
                                pic_displayed = True
                            except:
                                pass
                        
                        # Fallback to URL
                        if not pic_displayed and profile_data.get("profile_pic_url"):
                            try:
                                st.image(profile_data["profile_pic_url"], width=200)
                                pic_displayed = True
                            except:
                                pass
                        
                        if not pic_displayed:
                            st.info("üì∑ Profile picture not available")
                    else:
                        st.info("üì∑ Profile picture not scraped yet. Run scrape_profiles.py")
                    
                    # Basic stats
                    st.metric("Followers", f"{page_data.get('follower_count', 0):,}")
                    st.metric("Clients Following", len(page_data.get("clients_following", [])))
                
                with col2:
                    st.markdown(f"### @{username}")
                    st.markdown(f"**{page_data.get('full_name', 'N/A')}**")
                    
                    # Bio
                    if profile_data and isinstance(profile_data, dict) and profile_data.get("bio"):
                        st.markdown("**Bio:**")
                        st.markdown(profile_data["bio"])
                    else:
                        st.info("Bio not available. Run scrape_profiles.py")
                
                # Recent posts
                st.markdown("---")
                st.markdown("### Recent Posts")
                
                show_captions = st.checkbox("Show Captions", value=False)
                
                if profile_data and isinstance(profile_data, dict) and profile_data.get("posts"):
                    posts = profile_data["posts"][:12]  # Show up to 12 posts
                    
                    # Display in grid
                    cols = st.columns(3)
                    for idx, post in enumerate(posts):
                        col_idx = idx % 3
                        with cols[col_idx]:
                            # Try base64 first (doesn't expire), fallback to URL
                            image_displayed = False
                            if post.get("image_base64") and post.get("image_mime_type"):
                                try:
                                    # Display base64 image
                                    data_uri = f"data:{post['image_mime_type']};base64,{post['image_base64']}"
                                    st.image(data_uri, use_container_width=True)
                                    image_displayed = True
                                except Exception as e:
                                    # Fallback to URL if base64 fails
                                    pass
                            
                            # Fallback to URL if base64 not available
                            if not image_displayed and post.get("image_url"):
                                try:
                                    st.image(post["image_url"], use_container_width=True)
                                    image_displayed = True
                                except Exception as e:
                                    st.error("Image unavailable (URL expired)")
                            
                            if not image_displayed:
                                st.info("üì∑ Image not available")
                            
                            if show_captions and post.get("caption"):
                                caption = post["caption"][:100] + "..." if len(post.get("caption", "")) > 100 else post.get("caption", "")
                                st.caption(caption)
                else:
                    st.info("üì∏ Posts not scraped yet. Run scrape_profiles.py")
                
                # Category selection
                st.markdown("---")
                st.markdown("### Select Category")
                
                current_category = page_data.get("category", "UNKNOWN")
                category_options = ["UNKNOWN"] + list(CATEGORIES.keys())
                
                selected = st.selectbox(
                    "Category",
                    category_options,
                    index=category_options.index(current_category) if current_category in category_options else 0,
                    key=f"category_{username}"
                )
                
                # Store selection
                st.session_state.selected_category[username] = selected
                
                # Show category description
                if selected != "UNKNOWN" and selected in CATEGORIES:
                    st.info(f"**{selected.replace('_', ' ')}**: {CATEGORIES[selected]}")
            else:
                # No pages to display
                st.info("No pages to display. Adjust your filters or add more pages.")
    
    # TAB 2: Edit Page Details
    with tab2:
        try:
            st.header("Edit Page Details")
            
            # Search/select page
            pages = data.get("pages", {})
            page_usernames = sorted(pages.keys())
            
            if not page_usernames:
                st.info("No pages available yet. Run the scraper first.")
            else:
                # Handle quick-edit jumps
                if st.session_state.edit_page and st.session_state.edit_page in page_usernames:
                    st.session_state.edit_page_select = st.session_state.edit_page
                    st.session_state.edit_page = None
                
                # Jump-to-page controls
                jump_col1, jump_col2 = st.columns([3, 1])
                with jump_col1:
                    jump_input = st.text_input(
                        "Go directly to username",
                        value="",
                        key="page_jump_input",
                        placeholder="@username or username"
                    )
                with jump_col2:
                    if st.button("Go", key="page_jump_button"):
                        normalized = jump_input.strip().lstrip('@').lower()
                        match = next((u for u in page_usernames if u.lower() == normalized), None)
                        if match:
                            st.session_state.edit_page_select = match
                            st.success(f"Jumped to @{match}")
                            st.rerun()
                        else:
                            st.warning("Username not found in current dataset.")
                
                selected_username = st.selectbox(
                    "Select Page",
                    page_usernames,
                    key="edit_page_select",
                    help="Type to search or use the jump box above for exact usernames."
                )
                
                if selected_username:
                    page_data = pages[selected_username]
                    
                    st.markdown(f"### @{selected_username}")
                    st.markdown(f"**{page_data.get('full_name', 'N/A')}**")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown("#### Contact Information")
                        
                        # Known contact methods
                        known_methods = page_data.get("known_contact_methods", [])
                        selected_known = st.multiselect(
                            "Known Contact Methods",
                            CONTACT_METHODS,
                            default=known_methods,
                            key="known_methods"
                        )
                        
                        # Successful contact method
                        successful_method = page_data.get("successful_contact_method")
                        selected_successful = st.selectbox(
                            "Successful Contact Method",
                            [None] + CONTACT_METHODS,
                            index=0 if successful_method is None else CONTACT_METHODS.index(successful_method) + 1,
                            key="successful_method"
                        )
                        
                        # Current main contact method
                        current_main = page_data.get("current_main_contact_method")
                        selected_main = st.selectbox(
                            "Current Main Contact Method",
                            [None] + CONTACT_METHODS,
                            index=0 if current_main is None else CONTACT_METHODS.index(current_main) + 1,
                            key="main_method"
                        )
                        
                        # IG Account for DM (only show if IG DM is in known methods or is the main method)
                        ig_account_dm = page_data.get("ig_account_for_dm", "")
                        if "IG DM" in selected_known or selected_main == "IG DM":
                            ig_account_input = st.text_input(
                                "IG Account Used for DM",
                                value=ig_account_dm,
                                placeholder="e.g., @your_account",
                                key="ig_account_dm",
                                help="Which Instagram account username you use to DM this page"
                            )
                        else:
                            ig_account_input = ""
                    
                    with col2:
                        st.markdown("#### Pricing & Promo Status")
                        
                        # Promo price
                        promo_price = page_data.get("promo_price")
                        new_price = st.number_input(
                            "Promo Price ($)",
                            min_value=0.0,
                            value=float(promo_price) if promo_price else 0.0,
                            step=10.0,
                            key="promo_price"
                        )
                        
                        # Promo status (manual override)
                        current_promo_status = page_data.get("promo_status", "Unknown")
                        promo_status_options = ["Unknown", "Warm", "Not Open"]
                        selected_promo_status = st.selectbox(
                            "Promo Status",
                            promo_status_options,
                            index=promo_status_options.index(current_promo_status) if current_promo_status in promo_status_options else 0,
                            key="promo_status",
                            help="Manually set or override the promo status detected from bio/highlights"
                        )
                        
                        # Display current promo indicators
                        promo_indicators = page_data.get("promo_indicators", [])
                        if promo_indicators:
                            st.markdown("**Auto-detected Indicators:**")
                            for indicator in promo_indicators:
                                st.text(f"  ‚Ä¢ {indicator}")
                        
                        # Website URL
                        website_url = page_data.get("website_url", "")
                        website_input = st.text_input(
                            "Website URL",
                            value=website_url,
                            placeholder="https://example.com",
                            key="website_url",
                            help="Website URL from bio or link in bio (auto-detected during scraping)"
                        )
                        
                        # Website Promo Info (auto-detected)
                        website_promo_info = page_data.get("website_promo_info")
                        if website_promo_info:
                            st.markdown("**üåê Website Promo Detection:**")
                            if website_promo_info.get("has_promo_mention"):
                                st.success("‚úÖ Website mentions promo/advertising")
                            if website_promo_info.get("has_promo_page"):
                                st.success("‚úÖ Website has promo/buy page")
                                promo_urls = website_promo_info.get("promo_page_urls", [])
                                if promo_urls:
                                    for url in promo_urls[:3]:  # Show first 3
                                        st.text(f"  ‚Ä¢ {url}")
                            if website_promo_info.get("has_contact_email"):
                                st.success("‚úÖ Website has contact email for promo")
                                emails = website_promo_info.get("contact_emails", [])
                                if emails:
                                    for email in emails:
                                        st.text(f"  ‚Ä¢ {email}")
                            if website_promo_info.get("has_contact_form"):
                                st.success("‚úÖ Website has contact form")
                                forms = website_promo_info.get("contact_forms", [])
                                if forms:
                                    for form in forms[:3]:  # Show first 3
                                        form_details = []
                                        if form.get("has_email_field"):
                                            form_details.append("email field")
                                        if form.get("has_message_field"):
                                            form_details.append("message field")
                                        details_str = f" ({', '.join(form_details)})" if form_details else ""
                                        st.text(f"  ‚Ä¢ {form.get('url', 'N/A')}{details_str}")
                        
                        # Display current values
                        st.markdown("#### Current Values")
                        st.text(f"Category: {page_data.get('category', 'N/A')}")
                        st.text(f"Followers: {page_data.get('follower_count', 0):,}")
                        st.text(f"Clients Following: {len(page_data.get('clients_following', []))}")
                    
                    # Save button
                    if st.button("üíæ Save Changes", type="primary"):
                        page_data["known_contact_methods"] = selected_known
                        page_data["successful_contact_method"] = selected_successful
                        page_data["current_main_contact_method"] = selected_main
                        page_data["ig_account_for_dm"] = ig_account_input if ig_account_input else None
                        page_data["promo_price"] = new_price if new_price > 0 else None
                        page_data["promo_status"] = selected_promo_status
                        page_data["website_url"] = website_input if website_input else None
                        
                        save_data(data)
                        st.success("‚úÖ Changes saved!")
                        st.rerun()
        except Exception as e:
            st.error(f"Error in Edit Page Details tab: {str(e)}")
            import traceback
            st.code(traceback.format_exc())
    
    # TAB 4: View by Category (temporarily moved to last position)
    with tab4:
        try:
            st.header("View Pages by Category")
            
            # Category tabs
            category_list = ["UNCATEGORIZED"] + list(CATEGORIES.keys())
            category_tabs = st.tabs([cat.replace("_", " ").title() for cat in category_list])
            
            for idx, category in enumerate(category_list):
                with category_tabs[idx]:
                    pages_in_category = get_pages_by_category(data, category, min_clients=1)
                    
                    if not pages_in_category:
                        st.info(f"No pages found in category: {category}")
                        continue
                    
                    st.metric("Total Pages", len(pages_in_category))
                    
                    # Search/filter
                    search_term = st.text_input("üîç Search pages", key=f"search_{category}")
                    
                    # Display pages
                    for page_info in pages_in_category:
                        username = page_info["username"]
                        page_data = page_info["page_data"]
                        
                        # Filter by search
                        if search_term:
                            if search_term.lower() not in username.lower() and \
                               search_term.lower() not in page_data.get("full_name", "").lower():
                                continue
                        
                        with st.expander(f"@{username} - {page_data.get('full_name', 'N/A')}", expanded=False):
                            col1, col2, col3 = st.columns(3)
                            
                        with st.expander(f"@{username} - {page_data.get('full_name', 'N/A')}", expanded=False):
                            col1, col2, col3 = st.columns(3)
                            
                            with col1:
                                # Profile picture - try base64 first, fallback to URL
                                profile_data = page_data.get("profile_data")
                                if profile_data and isinstance(profile_data, dict):
                                    pic_displayed = False
                                    
                                    # Try base64 first
                                    if profile_data.get("profile_pic_base64") and profile_data.get("profile_pic_mime_type"):
                                        try:
                                            data_uri = f"data:{profile_data['profile_pic_mime_type']};base64,{profile_data['profile_pic_base64']}"
                                            st.image(data_uri, width=150)
                                            pic_displayed = True
                                        except:
                                            pass
                                    
                                    # Fallback to URL
                                    if not pic_displayed and profile_data.get("profile_pic_url"):
                                        try:
                                            st.image(profile_data["profile_pic_url"], width=150)
                                            pic_displayed = True
                                        except:
                                            pass
                                    
                                    if not pic_displayed:
                                        st.text("Image unavailable")
                                
                                # Instagram link
                                st.markdown(f"[üîó View on Instagram](https://instagram.com/{username})")
                            
                            with col2:
                                st.markdown("**Stats:**")
                                st.text(f"Followers: {page_info['follower_count']:,}")
                                st.text(f"Clients Following: {page_info['client_count']}")
                                st.text(f"Concentration: {page_info['concentration']:.2e}")
                                
                                if page_info["promo_price"]:
                                    st.text(f"Price: ${page_info['promo_price']:,.2f}")
                                    if page_info["concentration_per_dollar"]:
                                        st.text(f"Value: {page_info['concentration_per_dollar']:.2e} conc/$")
                            
                            with col3:
                                st.markdown("**Contact Info:**")
                                known = page_data.get("known_contact_methods", [])
                                if known:
                                    st.text(f"Known: {', '.join(known)}")
                                
                                successful = page_data.get("successful_contact_method")
                                if successful:
                                    st.text(f"‚úÖ Successful: {successful}")
                                
                                main = page_data.get("current_main_contact_method")
                                if main:
                                    st.text(f"üìû Current: {main}")
                                
                                ig_account = page_data.get("ig_account_for_dm")
                                if ig_account:
                                    st.text(f"üì± IG Account: {ig_account}")
                                
                                website = page_data.get("website_url")
                                if website:
                                    st.markdown(f"üåê [Website]({website})")
                                
                                # Show website promo info if available
                                website_promo = page_data.get("website_promo_info")
                                if website_promo:
                                    promo_flags = []
                                    if website_promo.get("has_promo_mention"):
                                        promo_flags.append("Mentions promo")
                                    if website_promo.get("has_promo_page"):
                                        promo_flags.append("Has promo page")
                                    if website_promo.get("has_contact_email"):
                                        promo_flags.append("Contact email")
                                    if website_promo.get("has_contact_form"):
                                        promo_flags.append("Contact form")
                                    if promo_flags:
                                        st.text(f"üåê Website: {', '.join(promo_flags)}")
                                
                                if not known and not successful and not main and not website:
                                    st.text("No contact info")
                            
                            # Quick edit button
                            if st.button(f"‚úèÔ∏è Quick Edit", key=f"edit_{username}_{category}"):
                                st.session_state.edit_page = username
                                st.rerun()
        except Exception as e:
            st.error(f"Error in View by Category tab: {str(e)}")
            import traceback
            st.code(traceback.format_exc())

    # TAB 3: Data Management (temporarily moved to 4th position to test)
    with tab3:
        st.header("üì§ Data Management")
        st.markdown("Upload your local `clients_data.json` to sync with Railway. VA's work will be preserved.")
        st.markdown("---")
        
        # Show current data status
        current_data = None
        try:
            current_data = load_data()
            if current_data:
                clients_count = len(current_data.get("clients", {}))
                pages_count = len(current_data.get("pages", {}))
                if clients_count > 0 or pages_count > 0:
                    st.info(f"üìä Current data: {clients_count} clients, {pages_count} pages")
                else:
                    st.warning("‚ö†Ô∏è No data currently loaded. Upload your `clients_data.json` file to get started.")
        except Exception as e:
            st.warning(f"Could not load current data: {str(e)}")
            import traceback
            st.code(traceback.format_exc())
        
        st.markdown("### üìÅ Upload File")
        st.markdown("**Method 1: File Uploader**")
        
        # File uploader
        uploaded_file = st.file_uploader(
            "Choose clients_data.json file",
            type=['json'],
            help="Upload your local clients_data.json file. It will be merged with existing data, preserving VA's categorization work.",
            key="data_upload_file"
        )
        
        if uploaded_file is not None:
            try:
                # Read uploaded file
                new_data = json.load(uploaded_file)
                
                # Load current data
                current_data = load_data()
                
                if current_data and (current_data.get("clients") or current_data.get("pages")):
                    st.info("üîÑ Merging uploaded data with existing data (preserving VA's work)...")
                    
                    # Merge data (preserve VA's work)
                    merged_data = merge_data_smart(current_data, new_data)
                    
                    # Save merged data
                    save_data(merged_data)
                    
                    st.success("‚úÖ Data uploaded and merged successfully! VA's work has been preserved.")
                    st.info("üîÑ Refresh the page to see updated data.")
                else:
                    # No existing data, just save the uploaded file
                    save_data(new_data)
                    st.success("‚úÖ Data uploaded successfully!")
                    st.info("üîÑ Refresh the page to see updated data.")
                    
            except json.JSONDecodeError:
                st.error("‚ùå Invalid JSON file. Please upload a valid clients_data.json file.")
            except Exception as e:
                st.error(f"‚ùå Error uploading file: {str(e)}")
                import traceback
                st.code(traceback.format_exc())
        
        st.markdown("---")
        st.markdown("### üîÑ Method 2: Paste JSON Data")
        st.markdown("If the file uploader doesn't work, paste your JSON data here:")
        
        json_text = st.text_area(
            "Paste your clients_data.json content here:",
            height=200,
            help="Copy the entire contents of your clients_data.json file and paste it here",
            key="json_paste_area"
        )
        
        if json_text.strip():
            if st.button("üì§ Upload Pasted JSON", key="upload_pasted_json"):
                try:
                    new_data = json.loads(json_text)
                    current_data = load_data()
                    
                    if current_data and (current_data.get("clients") or current_data.get("pages")):
                        merged_data = merge_data_smart(current_data, new_data)
                        save_data(merged_data)
                        st.success("‚úÖ Data uploaded and merged successfully! VA's work has been preserved.")
                        st.info("üîÑ Refresh the page to see updated data.")
                    else:
                        save_data(new_data)
                        st.success("‚úÖ Data uploaded successfully!")
                        st.info("üîÑ Refresh the page to see updated data.")
                except json.JSONDecodeError as e:
                    st.error(f"‚ùå Invalid JSON: {str(e)}")
                except Exception as e:
                    st.error(f"‚ùå Error: {str(e)}")
                    import traceback
                    st.code(traceback.format_exc())
        
        st.markdown("---")
        st.markdown("### üì• Download from Railway")
        st.markdown("Download the current Railway data to your local computer:")
        
        if st.button("üì• Download clients_data.json from Railway", key="download_railway_data"):
            try:
                from railway_sync import download_current_data
                railway_data = download_current_data()
                
                if railway_data:
                    # Save to local file
                    local_file = "clients_data.json"
                    with open(local_file, 'w', encoding='utf-8') as f:
                        json.dump(railway_data, f, indent=2, ensure_ascii=False)
                    
                    # Create download button
                    with open(local_file, 'r', encoding='utf-8') as f:
                        file_content = f.read()
                    
                    st.download_button(
                        label="üíæ Download File",
                        data=file_content,
                        file_name="clients_data.json",
                        mime="application/json",
                        key="download_button"
                    )
                    st.success(f"‚úÖ Downloaded {len(railway_data.get('clients', {}))} clients and {len(railway_data.get('pages', {}))} pages from Railway!")
                    st.info(f"üìÅ File saved to: {os.path.abspath(local_file)}")
                else:
                    st.warning("‚ö†Ô∏è Could not download from Railway. Make sure Railway CLI is installed and you're logged in.")
                    st.info("üí° Alternative: Use `railway run --service web cat /data/clients_data.json > clients_data.json` in your terminal")
            except ImportError:
                st.error("‚ùå railway_sync module not available")
            except Exception as e:
                st.error(f"‚ùå Download failed: {str(e)}")
                import traceback
                st.code(traceback.format_exc())
        
        st.markdown("---")
        st.markdown("### üîå API Sync Endpoint")
        st.markdown("Use this endpoint to upload data programmatically from your terminal:")
        
        # Display the sync endpoint URL
        sync_url = f"{os.environ.get('RAILWAY_PUBLIC_DOMAIN', 'localhost:8501')}"
        if not sync_url.startswith('http'):
            sync_url = f"https://{sync_url}"
        
        st.code(f"POST {sync_url}/?sync_data=true", language="bash")
        st.caption("Send JSON data in the request body to sync automatically.")
        
        # Handle API sync requests
        if st.query_params.get("sync_data") == "true":
            st.info("‚ö†Ô∏è API sync endpoint detected. This page is for manual upload only. Use a proper HTTP client for programmatic sync.")
        
        st.markdown("---")
        st.markdown("### üìä Current Data Status")
        data_file = get_data_file_path()
        if os.path.exists(data_file):
            file_size = os.path.getsize(data_file) / (1024 * 1024)  # MB
            st.info(f"üìÅ Data file: `{data_file}` ({file_size:.2f} MB)")
            
            data = load_data()
            if data:
                st.metric("Total Pages", len(data.get("pages", {})))
                st.metric("Total Clients", len(data.get("clients", {})))
        else:
            st.warning(f"‚ö†Ô∏è Data file not found at `{data_file}`")


def merge_data_smart(existing: Dict, new: Dict) -> Dict:
    """
    Smart merge: Preserves VA's work (categories, contact info) while updating scraped data
    """
    merged = {
        "clients": new.get("clients", {}),
        "pages": {}
    }
    
    existing_pages = existing.get("pages", {})
    new_pages = new.get("pages", {})
    
    for username, new_page_data in new_pages.items():
        existing_page_data = existing_pages.get(username, {})
        
        # Start with new scraped data
        merged_page = new_page_data.copy()
        
        # Preserve VA's categorization work
        if "category" in existing_page_data:
            merged_page["category"] = existing_page_data["category"]
        if "category_confidence" in existing_page_data:
            merged_page["category_confidence"] = existing_page_data["category_confidence"]
        
        # Preserve contact information (VA's edits)
        for field in [
            "known_contact_methods",
            "successful_contact_method",
            "current_main_contact_method",
            "ig_account_for_dm",
            "promo_price",
            "promo_status",  # Manual override
            "website_url"
        ]:
            if field in existing_page_data and existing_page_data[field]:
                merged_page[field] = existing_page_data[field]
        
        # Preserve website_promo_info if it exists
        if "website_promo_info" in existing_page_data:
            if "website_promo_info" not in new_page_data or not new_page_data.get("website_promo_info"):
                merged_page["website_promo_info"] = existing_page_data["website_promo_info"]
        
        merged["pages"][username] = merged_page
    
    # Add any pages that exist in old data but not in new
    for username, existing_page_data in existing_pages.items():
        if username not in merged["pages"]:
            merged["pages"][username] = existing_page_data
    
    return merged


if __name__ == "__main__":
    main()

